# -*- coding: utf-8 -*-
"""DataSetGeneration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IFPKQVfyAV1mGyOdOP-xDdZg-hWiwAc1
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split as tts

folder_name = 'CSCI544-Project/data/class_wise_embeddings_big'
folder_path = f'/content/drive/MyDrive/{folder_name}'
os.chdir(folder_path)
print("Current working directory is: " + os.getcwd())
print(os.listdir())

print(type(os.listdir()))

def get_balanced_dataset(filename):
    # Read the JSON file into a dataframe
    df = pd.read_json(filename, orient='records', lines=True)

    # Determine the minimum number of samples across the labels
    size_of_label = min(df['label'].value_counts())

    # Sample size_of_label rows for each label
    balanced_df = df.groupby('label').apply(lambda x: x.sample(size_of_label)).reset_index(drop=True)

    balanced_df = balanced_df.sample(frac=1).reset_index(drop=True)

    return balanced_df

# Example usage:
filename = 'bypass_embeddings.json'
balanced_data = get_balanced_dataset(filename)

balanced_data



def split_and_save(balanced_data, class_name: str):
    if not os.path.exists('./test_embeddings'):
        os.mkdir('./test_embeddings')

    # Check if the train_embeddings directory exists, if not, create it
    if not os.path.exists('./train_embeddings'):
        os.mkdir('./train_embeddings')
    # Split the dataset into training (80%) and testing (20%) sets
    train_df = balanced_data.sample(frac=0.8, random_state=42)
    test_df = balanced_data.drop(train_df.index)

    # Save the testing set as JSON
    test_df.to_json(f'./test_embeddings/{class_name}_test_file.json', orient='records', lines=True)

    # Split the training set into 4 equal parts
    num_train_files = 4
    train_files = np.array_split(train_df, num_train_files)

    # Save each part of the training set as separate JSON files
    for idx, train_file in enumerate(train_files, start=1):
        filename = f'./train_embeddings/{class_name}_train_file_{idx}.json'
        train_file.to_json(filename, orient='records', lines=True)

    print("Files saved successfully!")

# Example usage:
split_and_save(balanced_data, 'bypass')

for file in os.listdir():
    # Check if the file is a JSON file
    if file.endswith('.json'):
        # Extract the class name (the part before the underscore)
        class_name = file.split('_')[0]
        balanced_data = get_balanced_dataset(file)
        print(f'Shape of data {class_name}',balanced_data.shape)
        split_and_save(balanced_data, class_name)

purefilepath = '/content/drive/MyDrive/CSCI544-Project/t5p_small_embeddings/train/train.jsonl'

def get_balanced_dataset_other(pure_filename, impure_filename):
    # Read the JSON file into a dataframe
    df_pure = pd.read_json(pure_filename, orient='records', lines=True)
    df_impure = pd.read_json(impure_filename, orient='records', lines=True)

    pure_balanced_df = df_pure[df_pure['label'] == 0].sample(n=500, random_state=42).reset_index(drop=True)
    impure_balanced_df = df_impure[df_impure['label'] == 1].sample(n=500, random_state=42).reset_index(drop=True)

    balanced_df = pd.concat([pure_balanced_df,impure_balanced_df]).sample(frac=1).reset_index(drop=True)


    return balanced_df

# Example usage:

balanced_data = get_balanced_dataset_other(purefilepath, 'other_embeddings.json')

print(f'Shape of data other class',balanced_data.shape)
split_and_save(balanced_data, 'other')











