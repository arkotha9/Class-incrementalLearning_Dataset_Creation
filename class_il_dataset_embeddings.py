# -*- coding: utf-8 -*-
"""Class-IL_dataset_embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RwmImM3yUrXDUfeNG5zMy_nNq6xfEYkV
"""

import os
from google.colab import drive

drive.mount('/content/drive')
folder_name = 'NLP/Project/implementation/SVD'
folder_path = f'/content/drive/MyDrive/{folder_name}'
os.chdir(folder_path)
print("Current working directory is: " + os.getcwd())
print(os.listdir())

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np

def create_dataset(file_name):
  data_loaded = pd.read_csv(file_name, on_bad_lines='skip',index_col=0, header=0).dropna(subset=['Vulnerability Classification'])
  data_loaded['Vulnerability Classification'] = data_loaded['Vulnerability Classification'].str.strip().str.lower().str.replace(' ', '')
  print("Shape of the dataframe is " + str(data_loaded.shape))
  df = data_loaded[['func_before','Vulnerability Classification', 'vul']]

  value_counts = df['Vulnerability Classification'].value_counts()
  infrequent_classes = value_counts[value_counts < 1000].index
  df['Vulnerability Classification'] = df['Vulnerability Classification'].apply(lambda x: 'other' if x in (infrequent_classes) else x)

  return df

dataset = create_dataset('MSR_data_cleaned.csv') #removed nan values for vul type

"""#Generate class-incremental learning scenario"""

def create_task_datasets(df):

    vul_types = df['Vulnerability Classification'].unique()
    dfs_by_vul_type = {vul: pd.DataFrame(columns=df.columns) for vul in vul_types}

    # Randomly shuffle the rows and assign them to the corresponding DataFrames
    for vul in vul_types:
        vul_df = df.loc[df['Vulnerability Classification'] == vul, ['func_before', 'vul']]
        vul_df = vul_df.sample(frac=1)  # Randomly shuffle the rows
        dfs_by_vul_type[vul] = vul_df

    return dfs_by_vul_type

def create_task_datasets(df):
    vul_types = df['Vulnerability Classification'].unique()
    datasets_by_vul_type = {vul: [] for vul in vul_types}

    # Randomly shuffle the rows and assign them to the corresponding dictionaries
    for vul in vul_types:
        vul_df = df.loc[df['Vulnerability Classification'] == vul, ['func_before', 'vul']]
        vul_df = vul_df.sample(frac=1)  # Randomly shuffle the rows
        samples = vul_df.to_dict(orient='records')
        datasets_by_vul_type[vul] = samples

    return datasets_by_vul_type

#each key of this final_datasets correponds to single task dataset
final_datasets = create_task_datasets(dataset)

for vul_types in final_datasets.keys():
  print(len(final_datasets[vul_types]))

"""#Creatings emdeddings for each class"""

!pip install transformers

from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel
import torch

CHECKPOINT = "Salesforce/codet5p-110m-embedding"
DEVICE = "cuda"
TOKENIZER = AutoTokenizer.from_pretrained(CHECKPOINT, trust_remote_code=True)
MODEL = AutoModel.from_pretrained(CHECKPOINT, trust_remote_code=True).to(DEVICE)

#BASE_READ_PATH = "./class-embeddings/"
EMBEDDINGS_WRITE_PATH = "./t5p_small_class_embeddings/"

def get_code_embedding(code: str):
    inputs = TOKENIZER.encode(code, return_tensors="pt").to(DEVICE)
    embedding = MODEL(inputs)[0]
    embedding_np_array = embedding.reshape(-1, embedding.shape[0]).cpu().detach().numpy()
    del embedding
    return embedding_np_array

final_datasets.keys()

final_datasets['bypass'][90]

import json

def main(class_wise_data):
    # Create a directory to save the embeddings
    if not os.path.exists("./class_wise_embeddings"):
        os.makedirs("./class_wise_embeddings")

    for class_vul, samples in class_wise_data.items():
        save_path = os.path.join("class_wise_embeddings", f"{class_vul}_embeddings.json")

        with open(save_path, "w") as f_write:
          for i in range(len(samples)):
              if i % 30 == 0:
                  torch.cuda.empty_cache()
              df = samples[i] #has the func_before and vul labels
              code = df['func_before']
              embeddings = get_code_embedding(str(code))
              label = int(df['vul'])
              result = {"embeddings": embeddings.tolist(), "label": label}
              f_write.write(json.dumps(result) + "\n")

          print(f"Saved embeddings for '{class_vul}' to {save_path}")

main(final_datasets)

